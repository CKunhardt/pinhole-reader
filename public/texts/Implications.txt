Implications of Individual Differences on Evaluating Information Visualization Techniques
Ji Soo Yi Purdue University 315 N. Grant Street West Lafayette, IN 47907 USA yij@purdue.edu       

 Abstract.    

As HCI researchers extensively investigated the individual differences after the seminal work of Egan [6], the individual differences should gain more attention from researchers in information visualization (InfoVis). Though more evaluation studies have taken individual differences into consideration, it is unclear that corresponding measures are comprehensively and sufficiently understood. Thus, in this position paper, the author attempt to compile evidence showing that some individual differences may avoid our prior investigations and to suggest other potential measures to increase our coverage. The author also hoped that this position paper works as a call for creating a standardized measurement tool for individual differences. 


Keywords: individual difference, information visualization, 

1. Problem.     

After Egan’s seminal work [6], many researchers have investigated the influences of individual differences on task performances and human behaviors in human-computer interaction (HCI). The implications of individual difference in HCI were once well summarized by Dilon and Waston [4].
Information visualization (InfoVis) researchers have taken similar approaches. Velez et al. [13] investigated the impacts of five different cognitive abilities (visual memory, spatial orientation, spatial visualization, perceptual speed, and disembedding) on a basic visualization test, which is constructing a 3D model out of three orthogonal projects. They found that high spatial ability is correlated with the accuracy of their visualization test. Chen [1] also investigated four aspects (spatial ability, associative memory, visual memory, and online experiences) to see their influences on searching through a spatial-semantic virtual environment. Conati and Maclaren [3] conducted a rather comprehensive study to test six measures (i.e., visual memory, spatial visualization, perceptual speed, disembodiment, need for cognition, learning style (active/reflective, sensing/intuitive, visual/verbal, and sequential/global)) and found that only perceptual speed had a statistically significant effect on accuracy of given tasks using a visualization tool. Though it is not the focal point of their study, Douma et al. [5] included a questionnaire for sensory learning style [7], which showed that people who liked their system, SpicyNodes, tended to learn kinesthetically and remember visually. This result showed the relationships between cognitive learning style and their preferences toward an InfoVis tool. Most recently, Green et al. investigated the impacts of personality factors on interface learning performance [9]. They used six psychometric measures (the Locus of Control Inventory, the Beck Anxiety Inventory, the IPIP 20-item Big Five Neuroticism Scale, the IPIP 20-item Big Five Extroversion Scale, the Self-Regulation Scale, and the Scale of Intolerance-Tolerance of Ambiguity), and found that a subset of measures (9-item short measures) are correlated with performance measures.
However, it is unclear whether these studies cover all of influential individual differences. Ziemkiewicz and Kosara [14] showed that the level of internalization of metaphors (whether or not a person can describe the metaphor reliably) can influence their performance on different visualization tasks. However, it is not clear which cognitive ability or personal trait is corresponding to the level of internalization. The results of Kang et al. [10] also alluded that different participants used different strategies even while they used a same tool. In their study, for example, four participants using the same investigative analysis tool, Jigsaw, employed three different strategies in analyzing data. These variances may or may not be properly captured through previously investigated measures for individual differences. However, these variances are too huge to ignore. These variances can easily dilute the effects of other treatments in a controlled study. However, these characteristics do not seem to be well covered by previously investigated measures.   

2. Suggestions.    

In order to resolve this issue, the author proposes collective efforts to explore relevant measures for individual differences. Though Dillon and Watson [4] and Chen and Yu [2] reviewed ample candidates of measures, some of measures seem to largely avoid researchers’ attention so far. For example, some people are more easily influenced by environments and contexts, and this characteristic, called “fielddependence-independence,” has been well investigated in education and HCI (e.g., [11]). Participants who are field independent may not follow the guided approaches by a given tool and do whatever they want to do. This may make it difficult to observe the influence of different visualization tools in an evaluation study. In addition, a measure like opennessto-experience [12, 8] has not been thoroughly investigated, but they may have strong implications in understand how novice users understand unfamiliar visualization techniques. Visual literacy may provide us with greater insights, but it appears to be poorly defined and has not been used properly. In order to avoid confusion, we may need to separately define visualization literacy as a tool to measure how much a study participant know about various visualization techniques. For example, a person who knows how to interpret a parallel coordinates may perform differently with a complex multivariate information visualization technique.
After collecting meaningful measures, it would be important to publicly share these measures and corresponding measurement tools, so that lots of researchers can freely use these tools. However, as experiences in using them are accumulated, we should come up with ways to narrow down the number of survey questions and/or ways to select proper measures out of the comprehensive set depending on tasks and visualization techniques. As Conati and Maclaren [3] reported, conducting personality tests can take substantial time (about one hour), which could discourage the use of these tests.
The author hoped that this position paper inspire other InfoVis researchers to discuss a systematic approach to standardize measurement tools for individual differences.